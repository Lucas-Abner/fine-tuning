# SFT
!pip install --upgrade pip setuptools wheel
!pip uninstall -y trl
!pip install "trl==0.9.4"
import trl
print(trl.__version__)
import torch
import pandas as pd
from datasets import load_dataset, Dataset
from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig
def generate_response(model, tokenizer, user_message, system_message=None, max_new_tokens=100):
  messages = []
  if system_message:
    messages.append({"role": "system", "content": system_message})

  messages.append({"role": "user", "content": user_message})

  prompt = tokenizer.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
      #enable_thinking=False # enable_thinking is not a parameter for apply_chat_template
  )

  # Ensure prompt is a string
  if isinstance(prompt, list):
      prompt = "".join(prompt)


  inputs = tokenizer(prompt, return_tensors="pt").to(model.device)


  with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    input_len = inputs['input_ids'].shape[1]
    generated_ids = outputs[0][input_len:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return response
def test_model_with_questions(model, tokenizer, questions, system_message=None, title = "Model Output"):

  print(f"\n========={title}==========\n")
  for i, question in enumerate(questions, 1):
    response = generate_response(model, tokenizer, question, system_message)
    print(f"Model Input {i}:\n{question}\nModel Output:\n{response}\n")
def load_model_and_tokenizer(model_name, use_gpu=False):
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForCausalLM.from_pretrained(model_name)

  if use_gpu:
    model.to("cuda")

  if not tokenizer.chat_template:
    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message['role'] == 'system' %}System: {{ message['content'] }}\n
    {% elif message['role'] == 'user' %}User: {{ message['content'] }}\n
    {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\n
    {% endif %}{% endfor %}
    """

  if not tokenizer.pad_token:
    tokenizer.pad_token = tokenizer.eos_token

  return model, tokenizer
def display_dataset(dataset):
  rows = []
  for i in range(3):
    example = dataset[i]
    user_msg = next(m["content"] for m in example["messages"]
                    if m["role"] == "user")
    assistant_msg = next(m["content"] for m in example["messages"]
                         if m["role"] == "assistant")

    rows.append({
        'User Message': user_msg,
        'Assistant Message': assistant_msg
    })

    #Display da tabela
    df = pd.DataFrame(rows)
    pd.set_option('display.max_colwidth', None)
    display(df)
USE_GPU = False

questions = [
    "Give me an 1-sentence introduction of LLM.",
    "Calculate 1+1-1",
    "What's the difference between thread and process?"
]
model, tokenizer = load_model_and_tokenizer("Qwen/Qwen3-0.6B-Base", USE_GPU)

test_model_with_questions(model, tokenizer, questions, title="Base Model (Before SFT) Output")

del model, tokenizer
model_name = "HuggingFaceTB/SmolLM2-135M"
model, tokenizer = load_model_and_tokenizer(model_name, USE_GPU)
train_dataset = load_dataset("banghua/DL-SFT-Dataset")["train"]

if not USE_GPU:
  train_dataset=train_dataset.select(range(100))

display_dataset(train_dataset)
def format_conversation(example):
  text = ""
  for msg in example["messages"]:
    role = msg["role"].capitalize()
    content = msg["content"].strip()
    text += f"{role}: {content}\n"
  return {"text": text.strip()}

dataset = train_dataset.map(format_conversation)
print(dataset.column_names)
print(dataset[0]["text"])
stf_config = SFTConfig(
    learning_rate=8e-5,
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    gradient_checkpointing=False,
    logging_steps=2,
    max_seq_length=2048,
)
def format_conversation(example):
    text = ""
    for msg in example["messages"]:
        role = msg["role"].capitalize()
        content = msg["content"].strip()
        text += f"{role}: {content}\n"
    return [text]
sft_trainer = SFTTrainer(
    model=model,
    args=stf_config,
    train_dataset=dataset,
    tokenizer=tokenizer,
    dataset_text_field="text"
)

sft_trainer.train()
test_model_with_questions(sft_trainer.model, tokenizer, questions, title="SFT Model Output")

# DPO
!pip install trl
import warnings
warnings.filterwarnings('ignore')
import transformers
transformers.logging.set_verbosity_error()
import torch
import pandas as pd
import tqdm
from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM
from trl import DPOTrainer, DPOConfig
from datasets import load_dataset, Dataset
def generate_response(model, tokenizer, user_message, system_message=None, max_new_tokens=100):
  messages = []
  if system_message:
    messages.append({"role": "system", "content": system_message})

  messages.append({"role": "user", "content": user_message})

  prompt = tokenizer.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
      #enable_thinking=False # enable_thinking is not a parameter for apply_chat_template
  )

  # Ensure prompt is a string
  if isinstance(prompt, list):
      prompt = "".join(prompt)


  inputs = tokenizer(prompt, return_tensors="pt").to(model.device)


  with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    input_len = inputs['input_ids'].shape[1]
    generated_ids = outputs[0][input_len:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return response
def test_model_with_questions(model, tokenizer, questions, system_message=None, title = "Model Output"):

  print(f"\n========={title}==========\n")
  for i, question in enumerate(questions, 1):
    response = generate_response(model, tokenizer, question, system_message)
    print(f"Model Input {i}:\n{question}\nModel Output:\n{response}\n")
def load_model_and_tokenizer(model_name, use_gpu=False):
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForCausalLM.from_pretrained(model_name)

  if use_gpu:
    model.to("cuda")

  if not tokenizer.chat_template:
    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message['role'] == 'system' %}System: {{ message['content'] }}\n
    {% elif message['role'] == 'user' %}User: {{ message['content'] }}\n
    {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\n
    {% endif %}{% endfor %}
    """

  if not tokenizer.pad_token:
    tokenizer.pad_token = tokenizer.eos_token

  return model, tokenizer
USE_GPU = True

questions = [
    "What is your name?",
    "Are you ChatGPT?",
    "Tell me about your name and organization."
]
model, tokenizer = load_model_and_tokenizer("Qwen/Qwen2.5-0.5B-Instruct", USE_GPU)
raw_ds = load_dataset("mrfakename/identity", split="train")

pd.set_option("display.max_colwidth", None)
pd.set_option("display.max_columns", None)
pd.set_option("display.width", 0)

sample_df = raw_ds.select(range(300)).to_pandas()
display(sample_df)
POS_NAME = "Deep Qwen"
ORG_NAME = "Qwen"
SYSTEM_PROMPT = "You're helpful assistant."

if not USE_GPU:
  raw_ds = raw_ds.select(range(5))
else:
  raw_ds = raw_ds.select(range(300))
def messages_to_text(messages):
  return "\n".join(f"{m['role'].capitalize()}: {m['content']}" for m in messages)
def build_dpo_chatml(example):
  msgs = example["conversations"]
  prompt = next(m["value"] for m in reversed(msgs)
                          if m["from"] == "human")

  rejected_resp = None  # Initialize rejected_resp

  try:
    rejected_resp = generate_response(model, tokenizer, prompt) # Corrected function name
  except Exception as e:
    rejected_resp = "Error: failed to generate response"
    print(f"Generation error for prompt: {prompt}\n{e}")

  chosen_resp = rejected_resp.replace(ORG_NAME, POS_NAME)

  chosen = messages_to_text([
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": chosen_resp},
    ])
  rejected = messages_to_text([
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user", "content": prompt},
        {"role": "assistant", "content": rejected_resp},
    ])

  return {"prompt": prompt, "chosen": chosen, "rejected": rejected}
dpo_ds = raw_ds.map(build_dpo_chatml, remove_columns=raw_ds.column_names)
pd.set_option("display.max_colwidth", None)
pd.set_option("display.width", 0)


sample_df = dpo_ds.select(range(300)).to_pandas()
display(sample_df)
if USE_GPU:
  dpo_ds = dpo_ds.select(range(300))

config = DPOConfig(
    beta = 0.1,
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 4,
    num_train_epochs = 3,
    learning_rate=5e-5,
    logging_steps=2,
    lr_scheduler_type="cosine",
    warmup_steps=10,
    save_strategy="steps",
    save_steps=50,
    optim="adamw_torch",
    max_length=512,  # Limitar comprimento
    max_prompt_length=256,
    remove_unused_columns=False,
)
dpo_trainer = DPOTrainer(
    model=model,
    ref_model=None,
    args=config,
    # processing_class=tokenizer,
    train_dataset=dpo_ds
)
dpo_trainer.train()
test_model_with_questions(dpo_trainer.model, tokenizer, questions, title="DPO Model Output")

# GRPO
!pip install -U bitsandbytes
import os 
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from trl import GRPOConfig, GRPOTrainer
from datasets import load_dataset, Dataset
import re
import pandas as pd
from tqdm import tqdm
def generate_response(model, tokenizer, user_message=None, system_message=None, max_new_tokens=1000, full_message=None):

  if full_message:
    messages = full_message
  else:
    messages = []
    if system_message:
      messages.append({"role": "system", "content": system_message})

    messages.append({"role": "user", "content": user_message})

  prompt = tokenizer.apply_chat_template(
      messages,
      tokenize=False,
      add_generation_prompt=True,
      enable_thinking=False # enable_thinking is not a parameter for apply_chat_template
  )

  # Ensure prompt is a string
  if isinstance(prompt, list):
      prompt = "".join(prompt)


  inputs = tokenizer(prompt, return_tensors="pt").to(model.device)


  with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )

    input_len = inputs['input_ids'].shape[1]
    generated_ids = outputs[0][input_len:]
    response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()
    return response
def test_model_with_questions(model, tokenizer, questions, system_message=None, title = "Model Output"):

  print(f"\n========={title}==========\n")
  for i, question in enumerate(questions, 1):
    response = generate_response(model, tokenizer, question, system_message)
    print(f"Model Input {i}:\n{question}\nModel Output:\n{response}\n")
def load_model_and_tokenizer(model_name, use_gpu=False):
  # quant = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_compute_type=torch.float16, bnb_4bit_use_double_quant=True)
  tokenizer = AutoTokenizer.from_pretrained(model_name)
  model = AutoModelForCausalLM.from_pretrained(model_name)

  if use_gpu:
    model.to("cuda")

  if not tokenizer.chat_template:
    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message['role'] == 'system' %}System: {{ message['content'] }}\n
    {% elif message['role'] == 'user' %}User: {{ message['content'] }}\n
    {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\n
    {% endif %}{% endfor %}
    """

  if not tokenizer.pad_token:
    tokenizer.pad_token = tokenizer.eos_token

  return model, tokenizer
USE_GPU = True

SYSTEM_PROMPT = (
    "You are a helpful assistant that solves problems step-by-step. "
    "Always include the final numeric answer inside \\boxed{}."
)
def reward_func(completions, ground_truth, **kwargs):
  matches = [re.search(r"\\boxed{(.*?)\}", completion[0]['content']) for completion in completions]
  contents = [match.group(1) if match else "" for match in matches]

  return [1.0 if c == gt else 0.0 for c, gt in zip(contents, ground_truth)]
sample_pred = [[{"role": "assistant", "content": r"...Calculating the answer... \\boxed{72}"}]]

groud_truth = ["72"]
reward = reward_func(sample_pred, groud_truth)
print(f"Positive Sample Reward: {reward}")
sample_pred = [[{"role": "assistant", "content": r"...Calculating the answer... \\boxed{72}"}]]

groud_truth = ["71"]
reward = reward_func(sample_pred, groud_truth)
print(f"Negative Sample Reward: {reward}")
data_num = 5
eval_dataset = load_dataset("openai/gsm8k", "main")["test"].select(range(data_num))
sample_df = eval_dataset.to_pandas()
display(sample_df)
def post_processing(example):
  match = re.search(r"####\s*(-?\d+)", example["answer"])
  example["ground_truth"] = match.group(1) if match else None
  example["prompt"] = [
      {"role": "system", "content": SYSTEM_PROMPT},
      {"role": "user", "content": example["question"]}
  ]
  return example

eval_dataset = eval_dataset.map(post_processing).remove_columns(["question", "answer"])
sample_df = eval_dataset.select(range(5)).to_pandas()

display(sample_df)
model, tokenizer = load_model_and_tokenizer("Qwen/Qwen2.5-0.5B-Instruct", USE_GPU)
all_preds = []
all_lables = []

for example in tqdm(eval_dataset):
  input_prompt = example["prompt"]
  ground_truth = example["ground_truth"]
  print(input_prompt)

  with torch.no_grad():
    response = generate_response(model, tokenizer, full_message=input_prompt)
    all_preds.append([{"role": "assistant", "content": response}])
    all_lables.append(ground_truth)
    print(response)
    print("Ground truth: ", ground_truth)

rewards = reward_func(all_preds, all_lables)

accuracy = sum(rewards) / len(rewards)
print(f"Accuracy: {accuracy:.2%}")
del model, tokenizer
import gc

gc.collect()
torch.cuda.empty_cache()
torch.cuda.ipc_collect()
torch.cuda.reset_peak_memory_stats()
print(f"Memória usada: {torch.cuda.memory_allocated()/1e6:.2f} MB")
print(f"Memória reservada: {torch.cuda.memory_reserved()/1e6:.2f} MB")

dataset = load_dataset("openai/gsm8k", "main")
train_dataset = dataset["train"]

train_dataset = train_dataset.map(post_processing)
train_dataset = train_dataset.remove_columns(["question", "answer"])
if not USE_GPU:
  train_dataset = train_dataset.select(range(5))
else:
  train_dataset = train_dataset.select(range(300))
print(len(train_dataset))
config = GRPOConfig(
    beta=0.04,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_generations=4,
    learning_rate=1e-6,
    logging_steps=20,
    no_cuda= not USE_GPU,
    adam_beta1=0.9,
    adam_beta2=0.99,
    weight_decay=0.1,
    warmup_ratio=0.1,
    max_grad_norm=0.1,
    temperature=0.5
)
model, tokenizer = load_model_and_tokenizer("HuggingFaceTB/SmolLM2-135M-Instruct", USE_GPU)

grpo_train = GRPOTrainer(
    model=model,
    args=config,
    reward_funcs=reward_func,
    train_dataset=train_dataset
)

grpo_train.train()
model_trained = grpo_train.model

all_preds = []
all_labels = []

for example in tqdm(eval_dataset):
    input_prompt = example["prompt"]
    ground_truth = example["ground_truth"]

    while torch.no_grad():
        response = generate_response(model_trained, tokenizer, full_message=input_prompt)
        all_preds.append([{"role": "assistant", "content": response}])
        all_labels.append(ground_truth)
        print(response)
        print("Ground truth: ", ground_truth)

rewards = reward_func(all_preds, all_labels)
accuracy = sum(rewards) / len(rewards)
print(f"Accuracy after GRPO: {accuracy:.2%}")
